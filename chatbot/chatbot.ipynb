{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb79473-1d20-478a-8d8a-5063c35ef639",
   "metadata": {},
   "source": [
    "# Build a Chatbot powered by LLM\n",
    "To build a chatbot powered by LLM it would be important to remeber the history of conversations because LLM's are not context aware. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121a399a-4650-41ca-a01b-24f8627332c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41115b91-462b-443a-9926-55e5d090e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "# pip install -qU langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3052cbe5-ce6a-4730-a84c-2645182bebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "\n",
    "model = ChatMistralAI(model=\"mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a482b53-7f5f-4468-8e9c-e6fed17d225d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Bob! It's nice to meet you. How can I assist you today?\", response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 27, 'completion_tokens': 18}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-dc3ea5d4-773a-4576-a9ac-9477329d2e48-0', usage_metadata={'input_tokens': 9, 'output_tokens': 18, 'total_tokens': 27})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad1b97a-7b2f-47eb-8170-b093fdb03739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an assistant and I don't have the ability to know your name unless you tell me. Could you please tell me your name so I can assist you better?\", response_metadata={'token_usage': {'prompt_tokens': 9, 'total_tokens': 45, 'completion_tokens': 36}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-496a2895-2b22-4a16-b0a2-9633dd08b9c3-0', usage_metadata={'input_tokens': 9, 'output_tokens': 36, 'total_tokens': 45})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c402e7-a82a-4ac5-8b2a-e085b81d48fb",
   "metadata": {},
   "source": [
    "The model does not have state. It does not remember earlier conversation  \n",
    "Therefore we need to pass the complete conversation history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533b33e5-2227-4415-9b46-1e004c181c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on your introduction, your name is John.', response_metadata={'token_usage': {'prompt_tokens': 28, 'total_tokens': 38, 'completion_tokens': 10}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-970cc67d-ccea-466e-8b44-15a359938329-0', usage_metadata={'input_tokens': 28, 'output_tokens': 10, 'total_tokens': 38})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm John\"),\n",
    "        AIMessage(content=\"Hello John! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcbba8e-f0c4-4225-80d1-b5902c008b37",
   "metadata": {},
   "source": [
    "## Message History\n",
    "We need an object to track entire history of inputs and outputs so that it could be passed to the model every time  \n",
    "Fortunately langchain has a class for this  \n",
    "We have to install langchain-community to access those features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597272c-c629-4805-b8f3-d8fb2bcaca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8ae82f-9359-439e-aa70-6e52e3b328e1",
   "metadata": {},
   "source": [
    "After that, we can import the relevant classes and set up our chain which wraps the model and adds in this message history. A key part here is the function we pass into as the get_session_history. This function is expected to take in a session_id and return a Message History object. This session_id is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain (we'll show how to do that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca852d5d-d4a8-49b3-abd5-88a571f9c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858fbdce-955e-4061-98ae-90c1ccb550db",
   "metadata": {},
   "source": [
    "We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a session_id. This should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60a11541-31c0-40d5-aa68-1d397eed4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"001\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14dd4fe9-56ce-46a6-b269-a983d6902827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, Jake! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Jake\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8113077-81a4-4329-8f47-11db2911c77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on our conversation so far, your name is Jake.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b4660-5f94-4c4f-b4ca-8ef0f6748f1a",
   "metadata": {},
   "source": [
    "If we change the session id it will not have any history because history is linked to the session id.   \n",
    "see the code below as we change the session id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83cbd3fd-81c9-43fe-980b-397393bf2273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an assistant and I don't have personal information about individuals unless it has been shared with me in the course of our conversation. I'm here to help answer your questions to the best of my ability. If you'd like to tell me your name, I'll be happy to use it during our conversation.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"002\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570df12a-8640-41e7-be3c-b2279dc909cc",
   "metadata": {},
   "source": [
    "With the help of session id we can have chatbot support multiple users. each user can have it own session id. Also we can support multiple chats for each user with a different session id  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93678a40-1c74-4f6c-8a45-669bcc4cff5e",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "Next step is to add prompt templates. That will make our chatbot more accurate in terms of its responses\n",
    "We will use a message placeholder to pass our messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "601046cb-0046-43d5-a16e-278188a23b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98f44b-55a9-4abb-b406-b943171a2a11",
   "metadata": {},
   "source": [
    "Because we have used \"messages\" as the variable name of the place holder, we will pass our messages in the form of a dictionary with \"messages\" as key  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d230592-f5e9-4c5c-b3aa-efdad3a067ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Dalton! It's nice to meet you. How can I assist you today?\", response_metadata={'token_usage': {'prompt_tokens': 28, 'total_tokens': 47, 'completion_tokens': 19}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-a9b163c1-84a3-4d2f-a234-5125794f6b27-0', usage_metadata={'input_tokens': 28, 'output_tokens': 19, 'total_tokens': 47})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\": [HumanMessage(\"Hi, my name is Dalton\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab834f22-f9f6-4bf1-853a-071188283afc",
   "metadata": {},
   "source": [
    "Now lets wrap this chain in a chat history as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e82ddf-1391-4525-bc26-4b035f3902e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e8cfeb6-e889-481c-9860-2db26718f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"003\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "308fe28d-8131-4f92-a23e-0dd73eba4b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Elvis! I'm here to help you with any questions or information you need to the best of my ability. Just let me know what you need help with, and I'll do my best to provide you with accurate and helpful information.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Elvis\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7f5ce22-2416-4512-be5b-9bf871e4fdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d28f0-0df8-4d4a-9a3f-3bda02a4a163",
   "metadata": {},
   "source": [
    "We have made the prompt more complicated by including a language varaible in there  \n",
    "so the output will be in a different language what we specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ebf89f44-8449-46f8-b14f-681b889e06a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Bob! I'm here to help you. However, I must inform you that while I can understand and generate responses in Hindi, our conversation has started in English. If you'd like to continue in Hindi, feel free to ask your question again in Hindi, and I'll do my best to assist you in that language.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Hindi\"}\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5d3098-ee05-484a-abdb-84a711095e3b",
   "metadata": {},
   "source": [
    "Let's now wrap this more complicated chain in a Message History class. This time, because there are multiple keys in the input, we need to specify the correct key to use to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "087f6900-020d-4971-96c1-13714a04d915",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6e316d1-6482-4f67-8d73-dff57f448738",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"005\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "048dbb60-16ef-41bd-81d9-1f90e18085a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Namaste! Main aapke liye upayukt sahayak hoon. Mujhe yahi ummeed hai ki main aapke sawalon ka sarvottam tarika se jawab de sakta hoon. Kya aap koi sawal poochna chahte hain?\\n\\n(Hello! I am your helpful assistant. I hope I can answer your questions to the best of my ability. Would you like to ask me a question?)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm Devarsh\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7f5db95-72b9-46ce-b1cd-438f23461842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jab main aapko pehli baar se mila tha toh aapne apna naam Devarsh bataya tha, isliye main aapka naam Devarsh hi samajhta hoon.\\n\\n(When I met you for the first time, you told me your name was Devarsh, so I assume that your name is Devarsh.)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name?\")], \"language\": \"Hindi\"},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5cc4da-662a-418f-95e0-8d2955938185",
   "metadata": {},
   "source": [
    "## Size of Message History\n",
    "- Size should not exceed the LLM's max context size. Otherwise it will cause overflow of context window of LLM\n",
    "- Limit the size before the prompt step just after previous messages are loaded\n",
    "- use trim_messages function in langchain\n",
    "- trim_messages can control number of max tokens and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c32502eb-5516-47fe-91af-5dc1aa80caa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=35,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878f8a28-7735-4dd5-bd26-2a42f0e441c3",
   "metadata": {},
   "source": [
    "To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n",
    "\n",
    "Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee7fd551-892a-4261-a467-a9153199ea91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm glad you think I'm a helpful assistant! Unfortunately, I am not able to access personal information such as your name, and I am also unable to perform external tasks such as booking a flight ticket for you. My current capabilities are limited to providing information and answering questions to the best of my ability.\\nI can help you find some websites or travel agencies where you can look for cheapest non-stop flight from Mumbai to Toronto and also guide you on how to book it.\\nI apologize for any inconvenience.\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name? also I would like you to book a ticket for me from Mumbai to Torronto. The ticket should be cheapest and also in a non stop flight\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02a79777-40e1-4e6c-90f5-a215f29a480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm glad you think I'm a helpful assistant! Unfortunately, I am not able to access personal information such as your name, and I am also unable to perform external tasks such as booking a flight ticket for you. My current capabilities are limited to providing information and answering questions to the best of my ability.\\nI can help you find some websites or travel agencies where you can look for cheapest non-stop flight from Mumbai to Toronto and also guide you on how to book it.\\nI apologize for any inconvenience.\" response_metadata={'token_usage': {'prompt_tokens': 85, 'total_tokens': 197, 'completion_tokens': 112}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'} id='run-4e538063-8da2-4f53-9d4c-7b84d020f0aa-0' usage_metadata={'input_tokens': 85, 'output_tokens': 112, 'total_tokens': 197}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d44a83aa-ff63-4ad5-abca-317e301f4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c46a18e-afdf-4732-8672-80a80224e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7df2c2cb-105c-4b00-8f58-0b9952f02b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the information provided, I do not know your name.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e443e-12fc-4e00-ac6a-a70a362c5dad",
   "metadata": {},
   "source": [
    "## Streaming\n",
    "- LLM can take some time to send a response\n",
    "- for a better UX it will be good to stream the output of LLM token by token\n",
    "- All chains expose a .stream method, and ones that use message history are no different. We can simply use that method to get back a streaming response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3ac6bc38-c645-4bcb-acfa-81f22800ba76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hello| Todd|!| Nice| to| meet| you|.| Here|'|s| a| joke| for| you|:|\n",
      "\n",
      "Why| don|'|t| scientists| trust| atoms|?|\n",
      "\n",
      "Because| they| make| up| everything|!||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd407284-76ba-4442-b0d7-316025d10ed0",
   "metadata": {},
   "source": [
    "## Chatbot process flow\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "A[Message History] -->|store| B(Trimming)\n",
    "B --> C(Streaming)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fcd34d-4534-4af4-8f1f-554771e91c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
